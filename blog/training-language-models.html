<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>On the Art of Training Language Models - Chunyuan Deng</title>

  <meta name="author" content="Chunyuan Deng">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Renaissance-inspired fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Cormorant+Garamond:ital,wght@0,400;0,500;0,600;0,700;1,400&family=EB+Garamond:ital,wght@0,400;0,500;0,600;1,400&display=swap" rel="stylesheet">

  <style>
    :root {
      /* Morandi Color Palette */
      --morandi-cream: #f5f0e8;
      --morandi-warm-white: #faf7f2;
      --morandi-sage: #8a9a7b;
      --morandi-sage-dark: #6b7a5e;
      --morandi-dusty-rose: #c9a8a0;
      --morandi-terracotta: #b8917a;
      --morandi-muted-blue: #9aabb8;
      --morandi-warm-gray: #a69e94;
      --morandi-charcoal: #4a4541;
      --morandi-soft-brown: #7d6e63;
      /* Deep Renaissance colors */
      --renaissance-burgundy: #6b3a3a;
      --renaissance-forest: #3d4f3a;
      --renaissance-deep-brown: #3a3330;
      --renaissance-ochre: #8b6914;
      --renaissance-umber: #5c4033;
    }

    body {
      background: linear-gradient(135deg, var(--morandi-warm-white) 0%, var(--morandi-cream) 50%, #f0ebe3 100%);
      min-height: 100vh;
      font-family: 'EB Garamond', 'Georgia', serif;
      color: var(--morandi-charcoal);
      line-height: 1.8;
      padding: 20px;
    }

    a {
      color: var(--renaissance-forest);
      text-decoration: none;
      transition: all 0.3s ease;
      border-bottom: 1px solid transparent;
    }
    a:hover {
      color: var(--renaissance-burgundy);
      border-bottom: 1px solid var(--renaissance-burgundy);
    }

    .back-link {
      display: inline-block;
      margin-bottom: 30px;
      font-family: 'Cormorant Garamond', serif;
      font-weight: 600;
      letter-spacing: 0.5px;
    }

    .container {
      max-width: 750px;
      margin: 0 auto;
      background: linear-gradient(145deg, rgba(255,255,255,0.7) 0%, rgba(250,247,242,0.5) 100%);
      padding: 50px;
      border-radius: 8px;
      box-shadow: 0 4px 20px rgba(138, 154, 123, 0.12);
    }

    h1 {
      font-family: 'Cormorant Garamond', 'Georgia', serif;
      font-weight: 700;
      font-size: 36px;
      color: var(--renaissance-forest);
      letter-spacing: 1px;
      margin-bottom: 15px;
      line-height: 1.3;
    }

    .meta {
      font-family: 'Cormorant Garamond', serif;
      font-size: 14px;
      color: var(--morandi-soft-brown);
      margin-bottom: 35px;
      padding-bottom: 20px;
      border-bottom: 2px solid;
      border-image: linear-gradient(90deg, var(--renaissance-forest) 0%, var(--morandi-sage) 60%, transparent 100%) 1;
    }

    h2 {
      font-family: 'Cormorant Garamond', 'Georgia', serif;
      font-weight: 700;
      font-size: 24px;
      color: var(--renaissance-forest);
      margin-top: 35px;
      margin-bottom: 15px;
      letter-spacing: 0.5px;
    }

    p {
      margin-bottom: 20px;
      font-size: 17px;
    }

    em {
      font-style: italic;
      color: var(--morandi-soft-brown);
    }

    strong {
      color: var(--renaissance-forest);
      font-weight: 600;
    }

    blockquote {
      margin: 30px 0;
      padding: 20px 25px;
      background: linear-gradient(135deg, rgba(138, 154, 123, 0.08) 0%, rgba(107, 58, 58, 0.05) 100%);
      border-left: 4px solid var(--renaissance-forest);
      font-style: italic;
      color: var(--morandi-soft-brown);
    }

    @media screen and (max-width: 600px) {
      .container {
        padding: 30px 20px;
      }
      h1 {
        font-size: 28px;
      }
      h2 {
        font-size: 20px;
      }
      p {
        font-size: 15px;
      }
    }
  </style>
</head>

<body>
  <div class="container">
    <a href="../index.html" class="back-link">← Back to Home</a>

    <h1>On the Art of Training Language Models</h1>
    <div class="meta">January 2026 · Chunyuan Deng</div>

    <p>Training large language models often feels like an art form—a delicate dance between data quality, architecture choices, and computational constraints. Over the past year working on <strong>ByteFlow</strong>, I've come to appreciate how questioning fundamental assumptions can lead to elegant solutions that reshape our understanding of what's possible in language modeling.</p>

    <h2>The Tokenization Dilemma</h2>

    <p>Every major language model today relies on tokenization—breaking text into subword units before processing. It's so ubiquitous that we rarely question it. But should we? Tokenization introduces several challenges: vocabulary size limitations, language-specific biases, and the inability to handle novel character combinations gracefully.</p>

    <p>When we started exploring tokenization-free approaches, many colleagues were skeptical. "You'll never match the efficiency," they said. "The computational cost will be prohibitive." These concerns weren't unfounded—previous attempts at byte-level modeling had indeed struggled with these exact issues.</p>

    <h2>Learning from Raw Bytes</h2>

    <p>The key insight in ByteFlow was recognizing that we didn't need to process <em>every</em> byte with equal attention. By introducing <strong>adaptive compression</strong>, we could dynamically adjust how the model groups and processes byte sequences based on learned patterns. This allows the model to discover its own "tokenization" implicitly, optimized specifically for the task and data at hand.</p>

    <blockquote>
      "Sometimes the most elegant solutions come from questioning our fundamental assumptions about how things should work."
    </blockquote>

    <p>What fascinated me most wasn't just the performance—though ByteFlow did achieve competitive results with traditional tokenized models. It was watching the model learn to identify meaningful units of information on its own. In some languages, it discovered morpheme-like boundaries. In code, it learned to group common programming constructs. The model was teaching us what units of language truly matter.</p>

    <h2>The Philosophy of Model Design</h2>

    <p>This experience reinforced a philosophy I've come to embrace: <strong>interpretability should be a first-class citizen in model design</strong>, not an afterthought. When we design systems that are inherently more transparent in their operation, we gain insights that pure performance metrics can't provide.</p>

    <p>Tokenization-free approaches are just one example. There's immense potential in exploring other architectural choices that prioritize understanding alongside performance—sparse attention patterns, modular components, explicit memory mechanisms. Each of these offers different windows into how models process information.</p>

    <h2>Looking Forward</h2>

    <p>As models continue to grow in scale and capability, I believe the field needs to balance the pursuit of raw performance with the goal of building systems we can truly understand and trust. ByteFlow represents one step in that direction, but there's so much more to explore.</p>

    <p>The art of training language models isn't just about achieving state-of-the-art benchmarks. It's about discovering new ways to represent and process information, uncovering the fundamental principles that make these systems work, and building toward AI systems that are not just powerful, but also transparent and reliable.</p>

    <p style="margin-top: 40px; padding-top: 20px; border-top: 1px solid rgba(61, 79, 58, 0.2); color: var(--morandi-soft-brown); font-size: 15px;">
      <em>This post reflects thoughts developed during research on ByteFlow, published at ICLR 2026. For technical details, see the <a href="https://openreview.net/forum?id=GhJIa921j7">full paper</a>.</em>
    </p>

  </div>
</body>

</html>
