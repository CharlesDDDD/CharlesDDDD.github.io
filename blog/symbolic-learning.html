<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Understanding Symbolic Learning in Neural Networks - Chunyuan Deng</title>

  <meta name="author" content="Chunyuan Deng">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Renaissance-inspired fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Cormorant+Garamond:ital,wght@0,400;0,500;0,600;0,700;1,400&family=EB+Garamond:ital,wght@0,400;0,500;0,600;1,400&display=swap" rel="stylesheet">

  <style>
    :root {
      /* Morandi Color Palette */
      --morandi-cream: #f5f0e8;
      --morandi-warm-white: #faf7f2;
      --morandi-sage: #8a9a7b;
      --morandi-sage-dark: #6b7a5e;
      --morandi-dusty-rose: #c9a8a0;
      --morandi-terracotta: #b8917a;
      --morandi-muted-blue: #9aabb8;
      --morandi-warm-gray: #a69e94;
      --morandi-charcoal: #4a4541;
      --morandi-soft-brown: #7d6e63;
      /* Deep Renaissance colors */
      --renaissance-burgundy: #6b3a3a;
      --renaissance-forest: #3d4f3a;
      --renaissance-deep-brown: #3a3330;
      --renaissance-ochre: #8b6914;
      --renaissance-umber: #5c4033;
    }

    body {
      background: linear-gradient(135deg, var(--morandi-warm-white) 0%, var(--morandi-cream) 50%, #f0ebe3 100%);
      min-height: 100vh;
      font-family: 'EB Garamond', 'Georgia', serif;
      color: var(--morandi-charcoal);
      line-height: 1.8;
      padding: 20px;
    }

    a {
      color: var(--renaissance-forest);
      text-decoration: none;
      transition: all 0.3s ease;
      border-bottom: 1px solid transparent;
    }
    a:hover {
      color: var(--renaissance-burgundy);
      border-bottom: 1px solid var(--renaissance-burgundy);
    }

    .back-link {
      display: inline-block;
      margin-bottom: 30px;
      font-family: 'Cormorant Garamond', serif;
      font-weight: 600;
      letter-spacing: 0.5px;
    }

    .container {
      max-width: 750px;
      margin: 0 auto;
      background: linear-gradient(145deg, rgba(255,255,255,0.7) 0%, rgba(250,247,242,0.5) 100%);
      padding: 50px;
      border-radius: 8px;
      box-shadow: 0 4px 20px rgba(138, 154, 123, 0.12);
    }

    h1 {
      font-family: 'Cormorant Garamond', 'Georgia', serif;
      font-weight: 700;
      font-size: 36px;
      color: var(--renaissance-forest);
      letter-spacing: 1px;
      margin-bottom: 15px;
      line-height: 1.3;
    }

    .meta {
      font-family: 'Cormorant Garamond', serif;
      font-size: 14px;
      color: var(--morandi-soft-brown);
      margin-bottom: 35px;
      padding-bottom: 20px;
      border-bottom: 2px solid;
      border-image: linear-gradient(90deg, var(--renaissance-forest) 0%, var(--morandi-sage) 60%, transparent 100%) 1;
    }

    h2 {
      font-family: 'Cormorant Garamond', 'Georgia', serif;
      font-weight: 700;
      font-size: 24px;
      color: var(--renaissance-forest);
      margin-top: 35px;
      margin-bottom: 15px;
      letter-spacing: 0.5px;
    }

    p {
      margin-bottom: 20px;
      font-size: 17px;
    }

    em {
      font-style: italic;
      color: var(--morandi-soft-brown);
    }

    strong {
      color: var(--renaissance-forest);
      font-weight: 600;
    }

    code {
      background: rgba(138, 154, 123, 0.1);
      padding: 2px 6px;
      border-radius: 3px;
      font-family: 'Courier New', monospace;
      font-size: 15px;
      color: var(--renaissance-umber);
    }

    blockquote {
      margin: 30px 0;
      padding: 20px 25px;
      background: linear-gradient(135deg, rgba(138, 154, 123, 0.08) 0%, rgba(107, 58, 58, 0.05) 100%);
      border-left: 4px solid var(--renaissance-forest);
      font-style: italic;
      color: var(--morandi-soft-brown);
    }

    @media screen and (max-width: 600px) {
      .container {
        padding: 30px 20px;
      }
      h1 {
        font-size: 28px;
      }
      h2 {
        font-size: 20px;
      }
      p {
        font-size: 15px;
      }
    }
  </style>
</head>

<body>
  <div class="container">
    <a href="../index.html" class="back-link">← Back to Home</a>

    <h1>Understanding Symbolic Learning in Neural Networks</h1>
    <div class="meta">December 2025 · Chunyuan Deng</div>

    <p>Can neural networks truly understand symbols? This question has haunted the AI community since the early debates between connectionism and symbolic AI. Our recent work on arithmetic reasoning in language models suggests a surprising answer: yes, but not in the way we expected.</p>

    <h2>The Pattern Matching Myth</h2>

    <p>When large language models started solving arithmetic problems, the common explanation was straightforward: they're just pattern matching. The models memorize countless examples of "2 + 2 = 4" and similar equations during training, then retrieve and interpolate these patterns during inference.</p>

    <p>This explanation is compelling in its simplicity. It aligns with our intuition about neural networks as statistical machines that excel at finding correlations in data. But when we looked closer at how models actually perform arithmetic, we found something different—something more structured and, dare I say, more <em>symbolic</em>.</p>

    <h2>Evidence for Symbolic Reasoning</h2>

    <p>Through systematic probing experiments, we discovered that language models develop <strong>internal representations that align with mathematical operations</strong>. When processing <code>347 + 589</code>, the model doesn't just pattern match—it constructs intermediate representations corresponding to carrying operations, digit-wise addition, and place-value understanding.</p>

    <p>The key evidence came from analyzing how models generalize. If the model were purely pattern-matching, it would struggle with arithmetic operations involving numbers it hadn't seen during training. But we found that models trained on smaller numbers could reliably perform operations on larger ones, suggesting they learned the <em>algorithm</em> rather than memorizing examples.</p>

    <blockquote>
      "The distinction between pattern matching and symbolic reasoning isn't binary—it's a spectrum, and language models are further along that spectrum than we thought."
    </blockquote>

    <h2>What Makes This Symbolic?</h2>

    <p>Traditional symbolic systems manipulate discrete symbols according to explicit rules. Neural networks operate on continuous representations through learned transformations. What we're seeing in language models is a hybrid: <strong>learned symbolic reasoning</strong>.</p>

    <p>The models discover symbol-like representations and operation-like transformations through training, without being explicitly programmed with these concepts. The symbols aren't as crisp as in classical AI systems, but they're not mere statistical correlations either. They occupy a middle ground that challenges our categorical thinking about neural versus symbolic processing.</p>

    <h2>Implications for Interpretability</h2>

    <p>This has profound implications for AI interpretability and safety. If models can learn symbolic reasoning, we need tools to identify and verify these learned structures. Simply treating neural networks as black boxes that pattern-match becomes insufficient—and potentially dangerous—when these systems are making decisions based on learned logical operations.</p>

    <p>The good news is that symbolic learning might be easier to interpret than pure pattern matching. Symbols and operations have structure we can analyze, probe, and validate. This opens new avenues for mechanistic interpretability work, where we can ask questions like: "What symbolic operations has this model learned?" and "How reliable are these learned algorithms?"</p>

    <h2>Future Directions</h2>

    <p>Understanding symbolic learning in neural networks is just the beginning. Key questions remain:</p>

    <p><strong>Robustness:</strong> How stable are these learned symbolic representations? Do they break down under distribution shift or adversarial examples?</p>

    <p><strong>Compositionality:</strong> Can models combine learned symbolic operations in novel ways, or are they limited to the specific combinations seen during training?</p>

    <p><strong>Emergence:</strong> At what scale and under what conditions do these symbolic capabilities emerge? Can we design architectures that encourage more interpretable symbolic learning?</p>

    <p>These questions matter not just for understanding current systems, but for building the next generation of AI that's both capable and interpretable. The line between neural and symbolic isn't as clear as we once thought—and that's actually good news for building AI systems we can understand and trust.</p>

    <p style="margin-top: 40px; padding-top: 20px; border-top: 1px solid rgba(61, 79, 58, 0.2); color: var(--morandi-soft-brown); font-size: 15px;">
      <em>This post discusses findings from "Language Models are Symbolic Learners in Arithmetic," published in TMLR 2026. Read the <a href="https://arxiv.org/abs/2410.15580">full paper</a> for technical details and experimental results.</em>
    </p>

  </div>
</body>

</html>
