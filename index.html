<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Chunyuan Deng</title>
  
  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
<link rel="icon" href="images/GTVertical_TechGold.png">	
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Chunyuan Deng ÈÇìÊ∑≥Ëøú</name>
              </p>
              <p>I am a first-year CS PhD Student at <a href="https://www.rice.edu/">Rice University</a> advised by Prof. <a href="https://hanjiechen.github.io/">Hanjie Chen</a>. Previously, I am also affiliated with <a href="https://yale-nlp.github.io/">Yale NLP</a> and <a href="https://www.gatech.edu/">Georgia Tech</a>. </p>
		<p> My research interests primarily lie in the understanding of language models.   
              </p>
              <p style="text-align:center">
		<a href="https://scholar.google.com/citations?user=g7Y0RHcAAAAJ&hl=en&oi=ao">Google Scholar</a> &nbsp/&nbsp
		<a href="https://twitter.com/ChunyuanDeng">Twitter/X</a> &nbsp
		<a href="https://github.com/CharlesDDDD">Github</a> &nbsp
                <a href="mailto:cd110@rice.edu">Email</a> &nbsp/&nbsp
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/Chunyuan.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Chunyuan.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>üìú News</heading>
            <p>
                <ul>
                    <li>[Dec 2024] Paper on "Language Models are Symbolic Learners in Arithmetic" is now available on arXiv.</li>
                    <li>[Oct 2024] Started PhD at Rice University under Prof. Hanjie Chen's supervision.</li>
                    <li>[2024] Serving as a reviewer for multiple conferences including AISTATS 2025, ICLR 2025, NeurIPS 2024, COLM 2024, and ACL 2024.</li>
                    <li>[2024] Joined ACL Rolling Review as a reviewer.</li>
                </ul>
            </p>
        </td>
    </tr>
</tbody></table>
<!--        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>üî¶ Research</heading>
              <p>
                I primarily focus on the mechanisms and interpretability of language models and I am highly motivated to understand the 'why' behind various phenomena that occur in real-world scenarios. Specifically, I am currently or potentially interested in the following aspects of LLMs:
		<ul>
	<li><strong>Model Architechture</strong>: How to explore new <strong>interpretable arch.</strong> given the same amount of computational resources. Are there any architectures, like Backpack LMs, that could achieve similar performance to transformers while being easier to reverse engineer?</li>
        <li><strong>Model Probing</strong>: How to access parametric knowledge in Language Models to understand issues related to contamination, temporal boundaries, and leverage these insights to enhance training dynamics.</li>
        <li><strong>Mech Interp</strong>: How to interpret LLMs' capabilities from the mechanistic perspective to understand their behavior (e.g., SAEs). </li>
             </ul>
		      
              </p>
            </td>
          </tr>
        </tbody></table> -->
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	<heading>üî¶ Selected Publication</heading>
		<p>A selection of work that represent my recent research style.</p>
<tr>
<!--               <div class="one">
                <div class="two" id='mira_image'>
                  <img src='images/illustration_b.drawio.png' width="160"></div>
              </div> -->   
            </td> 
	    <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2410.15580">
                <papertitle>Language Models are Symbolic Learners in Arithmetic</papertitle>
              </a>
              <br>
              <strong>Chunyuan Deng</strong>, Zhiqi Li, Roy Xie, Ruidi Chang, Hanjie Chen
              <br>
           <em>Preprint 2024.  </em>
<!--               <p>We confirm that LMs are pure symbolic learners in arithmetic.</p> -->
	 </td></tr>
<!-- 	  <tr>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2311.09783">
                <papertitle>Investigating Data Contamination in Modern Benchmarks for Large Language Models</papertitle>
              </a>
              <br>
              <strong>Chunyuan Deng</strong>, Yilun Zhao, Xiangru Tang, Mark Gerstein, Arman Cohan
              <br>
           <em>NAACL 2024 (long papers). | Short Version in NeurIPS 2023 Workshop on on Backdoors in Deep Learning</em>
              <p>We present <strong>TS-Guessing</strong>: a method to probe LMs' internal knowledge by masking the portion of the testset/benchmark.</p>
            </td>
  </tr>     
 -->

	  
        </tbody>
  </table> 


	
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">

        <tr>
            <td>
                <heading>üõ†Ô∏è Service</heading>
                <p>
                    <ul>
                        <li><strong>Program Committee/Reviewer</strong>: AISTATS 2025, ICLR 2025, NeurIPS 2024, COLM 2024, ACL 2024, NAACL 2024, ACL Rolling Review (2023 - now), EMNLP 2024 BlackboxNLP Workshop, ICLR 2024 LLMAgents Workshop, NAACL 2024 Student Research Workshop. </li>
                    </ul>
                </p>
            </td>
        </tr>

</table>


			
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td>
              <heading>üéÑ Miscellaneous</heading>
	       <p>In my free time, I enjoy walking around the city photographing magnificent architecture, gorgeous scenery, and lovely interactions between human beings. üåá</p>
            </td>
          </tr>
       </table>
</body>

</html>
